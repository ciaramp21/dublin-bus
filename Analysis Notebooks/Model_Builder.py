#!/usr/bin/env python
# coding: utf-8



import pandas as pd
import pickle

# Sklearn tools

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import MinMaxScaler





# Read in the cleaned csv generated by the analysis file
df = pd.read_csv(r'Data\dummy_journeys.csv', keep_default_na=True, sep=',\s+', delimiter=',', skipinitialspace=True)





# Drop ROUTEID which has string characters
df.drop(['Unnamed: 0'], axis=1, inplace=True)
df.drop(['ROUTEID'], axis=1, inplace=True)

# Drop negative distances and zero stop journeys
df = df[df['DIST_BETWEEN'] > 1]  
df = df[df['STOPS_BETWEEN'] > 1] 




## Scaling data - Do this before the data is filtered the scale should still work regardless
col_names = ['DIST_BETWEEN', 'STOPS_BETWEEN', 'temp']
features = df[col_names]
scaler = MinMaxScaler().fit(features.values)
features = scaler.transform(features.values)
df[col_names] = features





line_ids = df['LINEID'].unique()
line_ids.shape





# set up dummies features
df = pd.get_dummies(df, columns=['WEEKDAY', 'DIRECTION', 'HOUR', 'weather_main'], 
                    prefix = ['WEEKDAY', 'DIRECTION', 'HOUR', 'weather_main'], drop_first=True)





for line_id in line_ids:

    # Create copy of df, and filter by line then drop that column
    df_line = df.copy()
    df_line = df_line.loc[df_line['LINEID'] == line_id]
    df_line = df_line.drop(columns=['LINEID'])
    
    # Data is already scaled and encoded
    y = pd.DataFrame(df_line["JOURNEY_TIME"])
    X = df_line.drop(["JOURNEY_TIME"],1)
    
    # Fit the model
    model = GradientBoostingRegressor(n_estimators=500, max_depth=5).fit(X, y)
    
    # create the pickle
    filename = 'xgb_reg_model_' + line_id + '.sav'
    pickle.dump(model, open(filename, 'wb'))
    




